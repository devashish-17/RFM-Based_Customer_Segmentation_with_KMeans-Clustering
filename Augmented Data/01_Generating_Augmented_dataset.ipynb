{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53319c76-6c44-4f13-97f8-690afe91ebd2",
   "metadata": {},
   "source": [
    "Given our dataset with customer transactions, considering the characteristics and typical usage of various algorithms:\n",
    "\n",
    "### 1. **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "SMOTE is best for scenarios where you're dealing with imbalanced classes, particularly in classification problems. It generates synthetic samples based on the nearest neighbors of minority class samples. In the context of your dataset, which appears to be a transactional dataset with various features, SMOTE might not be the most straightforward approach since it's usually applied to target variables in classification problems. However, if you're interested in synthetic expansion and have categorical targets, it could be adapted.\n",
    "\n",
    "### 2. **Data Augmentation**\n",
    "Data augmentation methods like adding noise or jittering are useful when you want to introduce slight variations in the dataset without fundamentally changing it. For your transaction dataset, you could add noise to numerical features or create synthetic variations in descriptions or quantities. This could be useful for expanding the dataset in a more realistic way.\n",
    "\n",
    "### 3. **Bootstrapping**\n",
    "Bootstrapping involves resampling your existing data with replacement. This method is simple and effective for increasing the size of your dataset and maintaining its original characteristics. It's particularly useful if your data is not imbalanced and you want to maintain the original distribution.\n",
    "\n",
    "### 4. **Data Duplication**\n",
    "Simply duplicating your dataset multiple times can also increase its size, although it may not introduce new information or variability. This method is straightforward but might not add significant value if diversity is required in the augmented dataset.\n",
    "\n",
    "### Recommended Approach\n",
    "For your dataset, a combination of **Bootstrapping** and **Data Augmentation** could be most effective:\n",
    "\n",
    "1. **Bootstrapping**: To directly increase the size of your dataset by resampling.\n",
    "2. **Data Augmentation**: Add noise or create synthetic variations to en used in combination with other methods for additional expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f14b31e8-bbb3-4740-a94c-74290d342ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6.220121</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>0.671104</td>\n",
       "      <td>17841.767219</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>8.269406</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>1.896648</td>\n",
       "      <td>17852.202819</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>12.616279</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>2.863311</td>\n",
       "      <td>17836.108498</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>3.039003</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>2.232953</td>\n",
       "      <td>17840.941535</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>2.261968</td>\n",
       "      <td>1/12/10 8:26</td>\n",
       "      <td>3.438669</td>\n",
       "      <td>17873.472343</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description   Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER   6.220121   \n",
       "1    536365     71053                  WHITE METAL LANTERN   8.269406   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER  12.616279   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE   3.039003   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.   2.261968   \n",
       "\n",
       "    InvoiceDate  UnitPrice    CustomerID         Country  \n",
       "0  1/12/10 8:26   0.671104  17841.767219  United Kingdom  \n",
       "1  1/12/10 8:26   1.896648  17852.202819  United Kingdom  \n",
       "2  1/12/10 8:26   2.863311  17836.108498  United Kingdom  \n",
       "3  1/12/10 8:26   2.232953  17840.941535  United Kingdom  \n",
       "4  1/12/10 8:26   3.438669  17873.472343  United Kingdom  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(r'E:\\DBDA_CDAC\\Projects\\CDAC_project\\RetailData.csv')\n",
    "\n",
    "# Method 1: Add Gaussian noise to numeric columns\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noisy_data = data.copy()\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        noise = noise_level * data[col].std() * np.random.randn(len(data))\n",
    "        noisy_data[col] += noise\n",
    "    return noisy_data\n",
    "\n",
    "# Method 2: Bootstrap sampling\n",
    "def bootstrap_data(data, target_size):\n",
    "    current_size = len(data)\n",
    "    multiplier = target_size // current_size\n",
    "    remainder = target_size % current_size\n",
    "    \n",
    "    # Replicate the dataset\n",
    "    larger_dataset = pd.concat([data] * multiplier, ignore_index=True)\n",
    "    \n",
    "    # Add some additional samples using bootstrapping if needed\n",
    "    if remainder > 0:\n",
    "        additional_samples = resample(data, n_samples=remainder, replace=True)\n",
    "        larger_dataset = pd.concat([larger_dataset, additional_samples], ignore_index=True)\n",
    "    \n",
    "    return larger_dataset\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_data = add_noise(data)\n",
    "augmented_data = bootstrap_data(augmented_data, target_size=10000000)  # 1 crore rows\n",
    "\n",
    "# Save the augmented dataset\n",
    "# augmented_data.to_csv('augmented_dataset.csv', index=False)\n",
    "augmented_data.shape\n",
    "augmented_data.head()\n",
    "# augmented_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f29aec-d54a-4c90-bbc5-6ea0b004e3bc",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Loading the Dataset**\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "```\n",
    "- **`pandas`** and **`numpy`** are imported for data manipulation and numerical operations.\n",
    "- **`resample`** is imported from `sklearn.utils` to perform bootstrapping.\n",
    "- The dataset is loaded from a CSV file into a DataFrame named `data`.\n",
    "\n",
    "### 2. **Adding Gaussian Noise to Numeric Columns**\n",
    "```python\n",
    "def add_noise(data, noise_level=0.01):\n",
    "    noisy_data = data.copy()\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        noise = noise_level * data[col].std() * np.random.randn(len(data))\n",
    "        noisy_data[col] += noise\n",
    "    return noisy_data\n",
    "```\n",
    "- **`add_noise`** is a function that adds Gaussian noise to the numeric columns in your dataset.\n",
    "- **`data.copy()`** creates a copy of the original dataset to avoid modifying it directly.\n",
    "- **`select_dtypes(include=[np.number])`** selects only the numeric columns in the dataset.\n",
    "- **`np.random.randn(len(data))`** generates random values from a standard normal distribution for each row.\n",
    "- The noise is scaled by the standard deviation of each column (`data[col].std()`) and a **`noise_level`** parameter (set to 0.01 by default).\n",
    "- The noisy data is then returned.\n",
    "\n",
    "### 3. **Bootstrapping the Dataset**\n",
    "```python\n",
    "def bootstrap_data(data, target_size):\n",
    "    current_size = len(data)\n",
    "    multiplier = target_size // current_size\n",
    "    remainder = target_size % current_size\n",
    "    \n",
    "    # Replicate the dataset\n",
    "    larger_dataset = pd.concat([data] * multiplier, ignore_index=True)\n",
    "    \n",
    "    # Add some additional samples using bootstrapping if needed\n",
    "    if remainder > 0:\n",
    "        additional_samples = resample(data, n_samples=remainder, replace=True)\n",
    "        larger_dataset = pd.concat([larger_dataset, additional_samples], ignore_index=True)\n",
    "    \n",
    "    return larger_dataset\n",
    "```\n",
    "- **`bootstrap_data`** is a function that increases the dataset size to a specified `target_size`.\n",
    "- **`current_size`** is the current number of rows in the dataset.\n",
    "- **`multiplier`** calculates how many full copies of the dataset are needed to approach the target size.\n",
    "- **`remainder`** calculates how many additional rows are needed after replicating the dataset multiple times.\n",
    "- **`pd.concat([data] * multiplier, ignore_index=True)`** creates a larger dataset by replicating the original dataset `multiplier` times. `ignore_index=True` ensures that the index is reset.\n",
    "- If additional rows are needed, **`resample`** is used to perform bootstrapping on the dataset and generate `remainder` rows.\n",
    "- The final dataset is returned.\n",
    "\n",
    "### 4. **Combining Data Augmentation and Bootstrapping**\n",
    "```python\n",
    "# Augment the dataset\n",
    "augmented_data = add_noise(data)\n",
    "1ugmented_dat1 = bootstrap_data(augmented_data, target_size=20000000)  # 2 crore rows\n",
    "```\n",
    "- The dataset is first augmented by adding noise using the `add_noise` function.\n",
    "- Then, the augmented dataset is further expanded using the `bootstrap_data` function to reach a target size of 2 crore rows (20 million rows).\n",
    "\n",
    "### 5. **Saving the Augmented Dataset**\n",
    "```python\n",
    "# Save the augmented dataset\n",
    "augmented_data.to_csv('augmented_dataset.csv', index=False)\n",
    "```\n",
    "- Finally, the augmented dataset is saved to a new CSV file named `augmented_dataset.csv`.\n",
    "- **`index=False`** ensures that the row indices are not included in the CSV file.\n",
    "\n",
    "### Summary of the Process:\n",
    "1. **Data Augmentation**: Adds Gaussian noise to numeric columns to introduce variability.\n",
    "2. **Bootstrapping**: Replicates and resamples the dataset to increase its size significantly.\n",
    "3. **Final Dataset**: Combines both methods to creing some variability to make the data more robust for machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
